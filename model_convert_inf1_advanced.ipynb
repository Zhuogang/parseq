{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/baudm_parseq_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# import torch.neuron\n",
    "import torch_neuron\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "preprocess_parseq = T.Compose([\n",
    "            T.Resize((32, 128), T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(0.5, 0.5)\n",
    "        ])\n",
    "\n",
    "parseq = torch.hub.load('baudm/parseq', 'parseq', pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cu102'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PurePath\n",
    "from typing import Sequence\n",
    "from torch import nn\n",
    "import yaml\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "chkpt_dir = \"/home/ubuntu/.cache/torch/hub/checkpoints/parseq-bb5792a6.pt\"\n",
    "checkpoint = torch.load(chkpt_dir, map_location=device)\n",
    "\n",
    "def _get_config(experiment=\"parseq\"):\n",
    "    \"\"\"Emulates hydra config resolution\"\"\"\n",
    "    root = PurePath(\"strhub/models/utils.py\").parents[2]\n",
    "    with open(root / 'configs/main.yaml', 'r') as f:\n",
    "        config = yaml.load(f, yaml.Loader)['model']\n",
    "    with open(root / f'configs/charset/94_full.yaml', 'r') as f:\n",
    "        config.update(yaml.load(f, yaml.Loader)['model'])\n",
    "    with open(root / f'configs/experiment/{experiment}.yaml', 'r') as f:\n",
    "        exp = yaml.load(f, yaml.Loader)\n",
    "    # Apply base model config\n",
    "    model = exp['defaults'][0]['override /model']\n",
    "    with open(root / f'configs/model/{model}_trace.yaml', 'r') as f:\n",
    "        config.update(yaml.load(f, yaml.Loader))\n",
    "    # Apply experiment config\n",
    "    if 'model' in exp:\n",
    "        config.update(exp['model'])\n",
    "    # Workaround for now: manually cast the lr to the correct type.\n",
    "    config['lr'] = float(config['lr'])\n",
    "    return config\n",
    "\n",
    "def _get_model_class(key):\n",
    "    if 'abinet' in key:\n",
    "        from strhub.models.abinet.system import ABINet as ModelClass\n",
    "    elif 'crnn' in key:\n",
    "        from strhub.models.crnn.system import CRNN as ModelClass\n",
    "    elif 'parseq' in key:\n",
    "        from strhub.models.parseq.system import PARSeq as ModelClass\n",
    "    elif 'trba' in key:\n",
    "        from strhub.models.trba.system import TRBA as ModelClass\n",
    "    elif 'trbc' in key:\n",
    "        from strhub.models.trba.system import TRBC as ModelClass\n",
    "    elif 'vitstr' in key:\n",
    "        from strhub.models.vitstr.system import ViTSTR as ModelClass\n",
    "    else:\n",
    "        raise InvalidModelError(\"Unable to find model class for '{}'\".format(key))\n",
    "    return ModelClass\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    config = _get_config(\"parseq\")\n",
    "    ModelClass = _get_model_class(\"parseq\")\n",
    "    model = ModelClass(**config).eval()\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "charset = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
    "BOS = '[B]'\n",
    "EOS = '[E]'\n",
    "PAD = '[P]'\n",
    "specials_first = (EOS,)\n",
    "specials_last = (BOS, PAD)\n",
    "itos = specials_first + tuple(charset) + specials_last\n",
    "stoi = {s: i for i, s in enumerate(itos)}\n",
    "eos_id, bos_id, pad_id = [stoi[s] for s in specials_first + specials_last]\n",
    "itos = specials_first + tuple(charset) + specials_last\n",
    "\n",
    "def tokenizer_filter(probs, ids):\n",
    "    ids = ids.tolist()\n",
    "    try:\n",
    "        eos_idx = ids.index(eos_id)\n",
    "    except ValueError:\n",
    "        eos_idx = len(ids)  # Nothing to truncate.\n",
    "    # Truncate after EOS\n",
    "    ids = ids[:eos_idx]\n",
    "    probs = probs[:eos_idx + 1]  # but include prob. for EOS (if it exists)\n",
    "    return probs, ids\n",
    "\n",
    "def ids2tok(token_ids):\n",
    "    tokens = [itos[i] for i in token_ids]\n",
    "    return ''.join(tokens)\n",
    "\n",
    "def decode(token_dists):\n",
    "    \"\"\"Decode a batch of token distributions.\n",
    "    Args:\n",
    "        token_dists: softmax probabilities over the token distribution. Shape: N, L, C\n",
    "        raw: return unprocessed labels (will return list of list of strings)\n",
    "\n",
    "    Returns:\n",
    "        list of string labels (arbitrary length) and\n",
    "        their corresponding sequence probabilities as a list of Tensors\n",
    "    \"\"\"\n",
    "    batch_tokens = []\n",
    "    batch_probs = []\n",
    "    for dist in token_dists:\n",
    "        probs, ids = dist.max(-1)  # greedy selection\n",
    "        probs, ids = tokenizer_filter(probs, ids)\n",
    "        tokens = ids2tok(ids)\n",
    "        batch_tokens.append(tokens)\n",
    "        batch_probs.append(probs)\n",
    "    return batch_tokens, batch_probs\n",
    "\n",
    "preprocess_parseq = T.Compose([\n",
    "            T.Resize((32, 128), T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(0.5, 0.5)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseq_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 32, 128])\n",
      "['448', '398', '464', '422', '464', '597', '3.34', '3.77'] ['448', '398', '464', '422', '464', '597', '3.34', '3.77']\n",
      "torch.Size([8, 3, 32, 128])\n",
      "['597', \"'13N\", '498', '647', '314', '4#', '224', '348'] ['597', \"'13N\", '498', '647', '314', '4#', '224', '348']\n",
      "torch.Size([8, 3, 32, 128])\n",
      "['464', '427', '698', '464', '458', '558', '1087', '464'] ['464', '427', '698', '464', '458', '558', '1087', '464']\n",
      "torch.Size([8, 3, 32, 128])\n",
      "['297', '472', '298', '98', '674', '387', '422', '467'] ['297', '472', '298', '98', '674', '387', '422', '467']\n",
      "torch.Size([8, 3, 32, 128])\n",
      "['538', '538', '547', '397', '474', '497', '997', '497'] ['538', '538', '547', '397', '474', '497', '997', '497']\n",
      "torch.Size([8, 3, 32, 128])\n",
      "['224', '417', '417', '538', '497', '494', '497', '$496'] ['224', '417', '417', '538', '497', '494', '497', '$496']\n",
      "torch.Size([8, 3, 32, 128])\n",
      "['1897', '5.00', '$224', '594', '382', '538', '697', '414'] ['1897', '5.00', '$224', '594', '382', '538', '697', '414']\n",
      "torch.Size([8, 3, 32, 128])\n",
      "['548', '498', '2087', '1547', '597', '1097', '$422', '448'] ['548', '498', '2087', '1547', '597', '1097', '$422', '448']\n",
      "torch.Size([8, 3, 32, 128])\n",
      "['747', '47', '548', '377', '647', '1997', '497', '497'] ['747', '47', '548', '377', '647', '1997', '497', '497']\n",
      "torch.Size([1, 3, 32, 128])\n",
      "['998'] ['998']\n"
     ]
    }
   ],
   "source": [
    "# model_parallel = torch.neuron.DataParallel(neuron_model)\n",
    "\n",
    "img_folder = \"/home/ubuntu/parseq/digits_demo\"\n",
    "batch_size = 8\n",
    "input_holder = torch.zeros(batch_size, 3, 32, 128)\n",
    "\n",
    "img_paths = [os.path.join(img_folder, x) for x in os.listdir(img_folder) if x.endswith(\"jpg\")]\n",
    "\n",
    "N = len(img_paths)\n",
    "n_batch = N//batch_size + 1\n",
    "\n",
    "for i in range(n_batch):\n",
    "    # print(list(range((i-1)*batch_size,batch_size*i)))\n",
    "    if batch_size*(i+1) < N:\n",
    "        input_holder = torch.zeros(batch_size, 3, 32, 128)\n",
    "        img_paths_tmp = img_paths[(i)*batch_size:batch_size*(i+1)]\n",
    "    else:\n",
    "        input_holder = torch.zeros(N-(n_batch-1)*batch_size, 3, 32, 128)\n",
    "        img_paths_tmp = img_paths[(i)*batch_size:N]\n",
    "    \n",
    "    print(input_holder.size())\n",
    "    for j in range(len(img_paths_tmp)):\n",
    "                \n",
    "        img_input = Image.open(img_paths_tmp[j]).convert('RGB')\n",
    "        img_input = preprocess_parseq(img_input.convert('RGB'))\n",
    "        # Preprocess. Model expects a batch of images with shape: (B, C, H, W)\n",
    "        \n",
    "        input_holder[j, :, :, :] = img_input[:,:,:]\n",
    "        \n",
    "    start = time.time()\n",
    "    logits_ts = parseq_model(input_holder)\n",
    "    end_1 = time.time()\n",
    "    logits = parseq(input_holder)\n",
    "    end_2 = time.time()\n",
    "\n",
    "    # print(\"parseq\")\n",
    "    # print(end_2-end_1)\n",
    "    # print(\"torchscript\")\n",
    "    # print(end_1-start)\n",
    "    pred = logits.softmax(-1)\n",
    "    label, confidence = decode(pred)\n",
    "        \n",
    "    pred = logits_ts.softmax(-1)\n",
    "    label_ts, confidence = decode(pred)\n",
    "    \n",
    "    print(label_ts, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARSeq(\n",
      "  original_name=PARSeq\n",
      "  (encoder): Encoder(\n",
      "    original_name=Encoder\n",
      "    (patch_embed): PatchEmbed(\n",
      "      original_name=PatchEmbed\n",
      "      (proj): Conv2d(original_name=Conv2d)\n",
      "      (norm): Identity(original_name=Identity)\n",
      "    )\n",
      "    (pos_drop): Dropout(original_name=Dropout)\n",
      "    (norm_pre): Identity(original_name=Identity)\n",
      "    (blocks): Sequential(\n",
      "      original_name=Sequential\n",
      "      (0): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "      (1): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "      (2): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "      (3): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "      (4): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "      (5): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "      (6): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "      (7): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "      (8): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "      (9): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "      (10): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "      (11): Block(\n",
      "        original_name=Block\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (attn): Attention(\n",
      "          original_name=Attention\n",
      "          (qkv): Linear(original_name=Linear)\n",
      "          (attn_drop): Dropout(original_name=Dropout)\n",
      "          (proj): Linear(original_name=Linear)\n",
      "          (proj_drop): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls1): Identity(original_name=Identity)\n",
      "        (drop_path1): Identity(original_name=Identity)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (mlp): Mlp(\n",
      "          original_name=Mlp\n",
      "          (fc1): Linear(original_name=Linear)\n",
      "          (act): GELU(original_name=GELU)\n",
      "          (drop1): Dropout(original_name=Dropout)\n",
      "          (fc2): Linear(original_name=Linear)\n",
      "          (drop2): Dropout(original_name=Dropout)\n",
      "        )\n",
      "        (ls2): Identity(original_name=Identity)\n",
      "        (drop_path2): Identity(original_name=Identity)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm(original_name=LayerNorm)\n",
      "    (fc_norm): Identity(original_name=Identity)\n",
      "    (head): Identity(original_name=Identity)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    original_name=Decoder\n",
      "    (layers): ModuleList(\n",
      "      original_name=ModuleList\n",
      "      (0): DecoderLayer(\n",
      "        original_name=DecoderLayer\n",
      "        (self_attn): MultiheadAttention(\n",
      "          original_name=MultiheadAttention\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(original_name=NonDynamicallyQuantizableLinear)\n",
      "        )\n",
      "        (cross_attn): MultiheadAttention(\n",
      "          original_name=MultiheadAttention\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(original_name=NonDynamicallyQuantizableLinear)\n",
      "        )\n",
      "        (linear1): Linear(original_name=Linear)\n",
      "        (dropout): Dropout(original_name=Dropout)\n",
      "        (linear2): Linear(original_name=Linear)\n",
      "        (norm1): LayerNorm(original_name=LayerNorm)\n",
      "        (norm2): LayerNorm(original_name=LayerNorm)\n",
      "        (norm_q): LayerNorm(original_name=LayerNorm)\n",
      "        (norm_c): LayerNorm(original_name=LayerNorm)\n",
      "        (dropout1): Dropout(original_name=Dropout)\n",
      "        (dropout2): Dropout(original_name=Dropout)\n",
      "        (dropout3): Dropout(original_name=Dropout)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm(original_name=LayerNorm)\n",
      "  )\n",
      "  (head): Linear(original_name=Linear)\n",
      "  (text_embed): TokenEmbedding(\n",
      "    original_name=TokenEmbedding\n",
      "    (embedding): Embedding(original_name=Embedding)\n",
      "  )\n",
      "  (dropout): Dropout(original_name=Dropout)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "output_path = \"parseq_cpu_trace.pt\"\n",
    "ts_model = parseq_model.to_torchscript(file_path=output_path, method=\"trace\", example_inputs = torch.rand(size=(1, 3, 32, 128)))\n",
    "\n",
    "print(ts_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch/jit/_trace.py:745: UserWarning: The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.\n",
      "  \"The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.\"\n",
      "INFO:Neuron:There are 119 ops of 4 different types in the TorchScript that are not compiled by neuron-cc: aten::view, aten::baddbmm, aten::embedding, aten::index_put_, (For more information see https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuron-cc/neuron-cc-ops/neuron-cc-ops-pytorch.html)\n",
      "INFO:Neuron:Number of arithmetic operators (pre-compilation) before = 1897, fused = 1524, percent fused = 80.34%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/ops/aten.py:2277: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:Compiling function _NeuronGraph$1337 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/0/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/0/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/0/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 3, 32, 128], \"float32\"]}, \"outputs\": [\"aten_detach/Const:0\", \"aten_cat/concat:0\", \"aten_slice_6/StridedSlice:0\", \"aten_slice_10/StridedSlice:0\", \"aten_slice_14/StridedSlice:0\", \"aten_slice_18/StridedSlice:0\", \"aten_slice_22/StridedSlice:0\", \"aten_slice_26/StridedSlice:0\", \"aten_slice_30/StridedSlice:0\", \"aten_slice_34/StridedSlice:0\", \"aten_slice_38/StridedSlice:0\", \"aten_slice_42/StridedSlice:0\", \"aten_slice_46/StridedSlice:0\", \"aten_slice_50/StridedSlice:0\", \"aten_slice_54/StridedSlice:0\", \"aten_slice_58/StridedSlice:0\", \"aten_slice_62/StridedSlice:0\", \"aten_slice_66/StridedSlice:0\", \"aten_slice_70/StridedSlice:0\", \"aten_slice_74/StridedSlice:0\", \"aten_slice_78/StridedSlice:0\", \"aten_slice_82/StridedSlice:0\", \"aten_slice_86/StridedSlice:0\", \"aten_slice_90/StridedSlice:0\", \"aten_slice_94/StridedSlice:0\", \"aten_slice_98/StridedSlice:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compile command returned: 1\n",
      "WARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$1337; falling back to native python function call\n",
      "ERROR:Neuron:neuron-cc failed with the following command line call:\n",
      "/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/0/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/0/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 3, 32, 128], \"float32\"]}, \"outputs\": [\"aten_detach/Const:0\", \"aten_cat/concat:0\", \"aten_slice_6/StridedSlice:0\", \"aten_slice_10/StridedSlice:0\", \"aten_slice_14/StridedSlice:0\", \"aten_slice_18/StridedSlice:0\", \"aten_slice_22/StridedSlice:0\", \"aten_slice_26/StridedSlice:0\", \"aten_slice_30/StridedSlice:0\", \"aten_slice_34/StridedSlice:0\", \"aten_slice_38/StridedSlice:0\", \"aten_slice_42/StridedSlice:0\", \"aten_slice_46/StridedSlice:0\", \"aten_slice_50/StridedSlice:0\", \"aten_slice_54/StridedSlice:0\", \"aten_slice_58/StridedSlice:0\", \"aten_slice_62/StridedSlice:0\", \"aten_slice_66/StridedSlice:0\", \"aten_slice_70/StridedSlice:0\", \"aten_slice_74/StridedSlice:0\", \"aten_slice_78/StridedSlice:0\", \"aten_slice_82/StridedSlice:0\", \"aten_slice_86/StridedSlice:0\", \"aten_slice_90/StridedSlice:0\", \"aten_slice_94/StridedSlice:0\", \"aten_slice_98/StridedSlice:0\"]}' --verbose 35\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/convert.py\", line 392, in op_converter\n",
      "    item, inputs, compiler_workdir=sg_workdir, **kwargs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/decorators.py\", line 229, in trace\n",
      "    'neuron-cc failed with the following command line call:\\n{}'.format(command))\n",
      "subprocess.SubprocessError: neuron-cc failed with the following command line call:\n",
      "/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/0/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/0/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 3, 32, 128], \"float32\"]}, \"outputs\": [\"aten_detach/Const:0\", \"aten_cat/concat:0\", \"aten_slice_6/StridedSlice:0\", \"aten_slice_10/StridedSlice:0\", \"aten_slice_14/StridedSlice:0\", \"aten_slice_18/StridedSlice:0\", \"aten_slice_22/StridedSlice:0\", \"aten_slice_26/StridedSlice:0\", \"aten_slice_30/StridedSlice:0\", \"aten_slice_34/StridedSlice:0\", \"aten_slice_38/StridedSlice:0\", \"aten_slice_42/StridedSlice:0\", \"aten_slice_46/StridedSlice:0\", \"aten_slice_50/StridedSlice:0\", \"aten_slice_54/StridedSlice:0\", \"aten_slice_58/StridedSlice:0\", \"aten_slice_62/StridedSlice:0\", \"aten_slice_66/StridedSlice:0\", \"aten_slice_70/StridedSlice:0\", \"aten_slice_74/StridedSlice:0\", \"aten_slice_78/StridedSlice:0\", \"aten_slice_82/StridedSlice:0\", \"aten_slice_86/StridedSlice:0\", \"aten_slice_90/StridedSlice:0\", \"aten_slice_94/StridedSlice:0\", \"aten_slice_98/StridedSlice:0\"]}' --verbose 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/ops/aten.py:3749: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:Compiling function _NeuronGraph$1338 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/51/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/51/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/51/graph_def.neff --io-config {\"inputs\": {}, \"outputs\": [\"aten_triu/Select:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_unsqueeze/ExpandDims:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_unsqueeze/ExpandDims:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_unsqueeze/ExpandDims:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_unsqueeze/ExpandDims:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_unsqueeze/ExpandDims:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_unsqueeze/ExpandDims:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_unsqueeze/ExpandDims:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_unsqueeze/ExpandDims:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1339 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/146/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/146/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/146/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[], \"int64\"], \"1:0\": [[1, 1], \"int64\"], \"2:0\": [[1, 1, 384], \"float32\"], \"3:0\": [[1, 0, 384], \"float32\"], \"4:0\": [[1, 2], \"int64\"], \"5:0\": [[1, 1, 384], \"float32\"], \"6:0\": [[1, 1, 384], \"float32\"], \"7:0\": [[1, 3], \"int64\"], \"8:0\": [[1, 1, 384], \"float32\"], \"9:0\": [[1, 2, 384], \"float32\"], \"10:0\": [[1, 4], \"int64\"], \"11:0\": [[1, 1, 384], \"float32\"], \"12:0\": [[1, 3, 384], \"float32\"], \"13:0\": [[1, 5], \"int64\"], \"14:0\": [[1, 1, 384], \"float32\"], \"15:0\": [[1, 4, 384], \"float32\"], \"16:0\": [[1, 6], \"int64\"], \"17:0\": [[1, 1, 384], \"float32\"], \"18:0\": [[1, 5, 384], \"float32\"], \"19:0\": [[1, 7], \"int64\"], \"20:0\": [[1, 1, 384], \"float32\"], \"21:0\": [[1, 6, 384], \"float32\"], \"22:0\": [[1, 8], \"int64\"], \"23:0\": [[1, 1, 384], \"float32\"], \"24:0\": [[1, 7, 384], \"float32\"]}, \"outputs\": [\"aten_expand/BroadcastTo:0\", \"TokenEmbedding_100/aten_slice_3/StridedSlice:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_190/aten_slice_3/StridedSlice:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_280/aten_slice_3/StridedSlice:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_370/aten_slice_3/StridedSlice:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_460/aten_slice_3/StridedSlice:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_550/aten_slice_3/StridedSlice:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_640/aten_slice_3/StridedSlice:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_730/aten_slice_3/StridedSlice:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compile command returned: 1\n",
      "WARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$1339; falling back to native python function call\n",
      "ERROR:Neuron:neuron-cc failed with the following command line call:\n",
      "/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/146/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/146/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[], \"int64\"], \"1:0\": [[1, 1], \"int64\"], \"2:0\": [[1, 1, 384], \"float32\"], \"3:0\": [[1, 0, 384], \"float32\"], \"4:0\": [[1, 2], \"int64\"], \"5:0\": [[1, 1, 384], \"float32\"], \"6:0\": [[1, 1, 384], \"float32\"], \"7:0\": [[1, 3], \"int64\"], \"8:0\": [[1, 1, 384], \"float32\"], \"9:0\": [[1, 2, 384], \"float32\"], \"10:0\": [[1, 4], \"int64\"], \"11:0\": [[1, 1, 384], \"float32\"], \"12:0\": [[1, 3, 384], \"float32\"], \"13:0\": [[1, 5], \"int64\"], \"14:0\": [[1, 1, 384], \"float32\"], \"15:0\": [[1, 4, 384], \"float32\"], \"16:0\": [[1, 6], \"int64\"], \"17:0\": [[1, 1, 384], \"float32\"], \"18:0\": [[1, 5, 384], \"float32\"], \"19:0\": [[1, 7], \"int64\"], \"20:0\": [[1, 1, 384], \"float32\"], \"21:0\": [[1, 6, 384], \"float32\"], \"22:0\": [[1, 8], \"int64\"], \"23:0\": [[1, 1, 384], \"float32\"], \"24:0\": [[1, 7, 384], \"float32\"]}, \"outputs\": [\"aten_expand/BroadcastTo:0\", \"TokenEmbedding_100/aten_slice_3/StridedSlice:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_190/aten_slice_3/StridedSlice:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_280/aten_slice_3/StridedSlice:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_370/aten_slice_3/StridedSlice:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_460/aten_slice_3/StridedSlice:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_550/aten_slice_3/StridedSlice:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_640/aten_slice_3/StridedSlice:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_730/aten_slice_3/StridedSlice:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\"]}' --verbose 35\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/convert.py\", line 392, in op_converter\n",
      "    item, inputs, compiler_workdir=sg_workdir, **kwargs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/decorators.py\", line 229, in trace\n",
      "    'neuron-cc failed with the following command line call:\\n{}'.format(command))\n",
      "subprocess.SubprocessError: neuron-cc failed with the following command line call:\n",
      "/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/146/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/146/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[], \"int64\"], \"1:0\": [[1, 1], \"int64\"], \"2:0\": [[1, 1, 384], \"float32\"], \"3:0\": [[1, 0, 384], \"float32\"], \"4:0\": [[1, 2], \"int64\"], \"5:0\": [[1, 1, 384], \"float32\"], \"6:0\": [[1, 1, 384], \"float32\"], \"7:0\": [[1, 3], \"int64\"], \"8:0\": [[1, 1, 384], \"float32\"], \"9:0\": [[1, 2, 384], \"float32\"], \"10:0\": [[1, 4], \"int64\"], \"11:0\": [[1, 1, 384], \"float32\"], \"12:0\": [[1, 3, 384], \"float32\"], \"13:0\": [[1, 5], \"int64\"], \"14:0\": [[1, 1, 384], \"float32\"], \"15:0\": [[1, 4, 384], \"float32\"], \"16:0\": [[1, 6], \"int64\"], \"17:0\": [[1, 1, 384], \"float32\"], \"18:0\": [[1, 5, 384], \"float32\"], \"19:0\": [[1, 7], \"int64\"], \"20:0\": [[1, 1, 384], \"float32\"], \"21:0\": [[1, 6, 384], \"float32\"], \"22:0\": [[1, 8], \"int64\"], \"23:0\": [[1, 1, 384], \"float32\"], \"24:0\": [[1, 7, 384], \"float32\"]}, \"outputs\": [\"aten_expand/BroadcastTo:0\", \"TokenEmbedding_100/aten_slice_3/StridedSlice:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_190/aten_slice_3/StridedSlice:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_280/aten_slice_3/StridedSlice:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_370/aten_slice_3/StridedSlice:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_460/aten_slice_3/StridedSlice:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_550/aten_slice_3/StridedSlice:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_640/aten_slice_3/StridedSlice:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\", \"TokenEmbedding_730/aten_slice_3/StridedSlice:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\"]}' --verbose 35\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1340 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/157/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/157/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/157/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_div/truediv:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1341 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/162/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/162/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/162/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[8, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1342 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/172/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/172/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/172/graph_def.neff --io-config {\"inputs\": {}, \"outputs\": [\"aten_triu/Select:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1343 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/199/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/199/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/199/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_div/truediv:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1344 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/204/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/204/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/204/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1345 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/222/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/222/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/222/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_div/truediv:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1346 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/227/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/227/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/227/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[2, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1347 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/245/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/245/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/245/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_div/truediv:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1348 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/250/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/250/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/250/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[3, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1349 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/268/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/268/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/268/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_div/truediv:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1350 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/273/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/273/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/273/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[4, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1351 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/291/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/291/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/291/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_div/truediv:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1352 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/296/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/296/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/296/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[5, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1353 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/314/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/314/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/314/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_div/truediv:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1354 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/319/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/319/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/319/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[6, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1355 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/337/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/337/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/337/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_div/truediv:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1356 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/342/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/342/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/342/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[7, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1357 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/350/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/350/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/350/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[8, 12, 32], \"float32\"], \"1:0\": [[12, 1, 8], \"float32\"]}, \"outputs\": [\"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1358 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/352/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/352/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/352/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"], \"1:0\": [[12, 1, 1], \"float32\"]}, \"outputs\": [\"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1359 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/354/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/354/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/354/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[2, 12, 32], \"float32\"], \"1:0\": [[12, 1, 2], \"float32\"]}, \"outputs\": [\"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1360 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/356/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/356/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/356/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[3, 12, 32], \"float32\"], \"1:0\": [[12, 1, 3], \"float32\"]}, \"outputs\": [\"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1361 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/358/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/358/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/358/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[4, 12, 32], \"float32\"], \"1:0\": [[12, 1, 4], \"float32\"]}, \"outputs\": [\"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1362 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/360/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/360/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/360/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[5, 12, 32], \"float32\"], \"1:0\": [[12, 1, 5], \"float32\"]}, \"outputs\": [\"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1363 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/362/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/362/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/362/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[6, 12, 32], \"float32\"], \"1:0\": [[12, 1, 6], \"float32\"]}, \"outputs\": [\"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1364 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/364/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/364/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/364/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[7, 12, 32], \"float32\"], \"1:0\": [[12, 1, 7], \"float32\"]}, \"outputs\": [\"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1365 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/366/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/366/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/366/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1366 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/370/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/370/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/370/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1367 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/374/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/374/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/374/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1368 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/378/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/378/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/378/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1369 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/382/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/382/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/382/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1370 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/386/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/386/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/386/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1371 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/390/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/390/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/390/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1372 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/394/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/394/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/394/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1373 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/398/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/398/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/398/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 3, 32, 128], \"float32\"], \"1:0\": [[1, 1, 384], \"float32\"], \"2:0\": [[1, 1, 384], \"float32\"], \"3:0\": [[1, 1, 384], \"float32\"], \"4:0\": [[1, 1, 384], \"float32\"], \"5:0\": [[1, 1, 384], \"float32\"], \"6:0\": [[1, 1, 384], \"float32\"], \"7:0\": [[1, 1, 384], \"float32\"], \"8:0\": [[1, 1, 384], \"float32\"], \"9:0\": [[1, 1, 384], \"float32\"], \"10:0\": [[1, 1, 384], \"float32\"], \"11:0\": [[1, 1, 384], \"float32\"], \"12:0\": [[1, 1, 384], \"float32\"], \"13:0\": [[1, 1, 384], \"float32\"], \"14:0\": [[1, 1, 384], \"float32\"], \"15:0\": [[1, 1, 384], \"float32\"], \"16:0\": [[1, 1, 384], \"float32\"]}, \"outputs\": [\"Encoder_12/LayerNorm_12/aten_layer_norm/batchnorm/add_1:0\", \"Decoder_132/DecoderLayer_3/aten_add/add:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_222/DecoderLayer_3/aten_add/add:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_312/DecoderLayer_3/aten_add/add:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_402/DecoderLayer_3/aten_add/add:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_492/DecoderLayer_3/aten_add/add:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_582/DecoderLayer_3/aten_add/add:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_672/DecoderLayer_3/aten_add/add:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_762/DecoderLayer_3/aten_add/add:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compile command returned: 1\n",
      "WARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$1373; falling back to native python function call\n",
      "ERROR:Neuron:neuron-cc failed with the following command line call:\n",
      "/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/398/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/398/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 3, 32, 128], \"float32\"], \"1:0\": [[1, 1, 384], \"float32\"], \"2:0\": [[1, 1, 384], \"float32\"], \"3:0\": [[1, 1, 384], \"float32\"], \"4:0\": [[1, 1, 384], \"float32\"], \"5:0\": [[1, 1, 384], \"float32\"], \"6:0\": [[1, 1, 384], \"float32\"], \"7:0\": [[1, 1, 384], \"float32\"], \"8:0\": [[1, 1, 384], \"float32\"], \"9:0\": [[1, 1, 384], \"float32\"], \"10:0\": [[1, 1, 384], \"float32\"], \"11:0\": [[1, 1, 384], \"float32\"], \"12:0\": [[1, 1, 384], \"float32\"], \"13:0\": [[1, 1, 384], \"float32\"], \"14:0\": [[1, 1, 384], \"float32\"], \"15:0\": [[1, 1, 384], \"float32\"], \"16:0\": [[1, 1, 384], \"float32\"]}, \"outputs\": [\"Encoder_12/LayerNorm_12/aten_layer_norm/batchnorm/add_1:0\", \"Decoder_132/DecoderLayer_3/aten_add/add:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_222/DecoderLayer_3/aten_add/add:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_312/DecoderLayer_3/aten_add/add:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_402/DecoderLayer_3/aten_add/add:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_492/DecoderLayer_3/aten_add/add:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_582/DecoderLayer_3/aten_add/add:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_672/DecoderLayer_3/aten_add/add:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_762/DecoderLayer_3/aten_add/add:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\"]}' --verbose 35\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/convert.py\", line 392, in op_converter\n",
      "    item, inputs, compiler_workdir=sg_workdir, **kwargs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/decorators.py\", line 229, in trace\n",
      "    'neuron-cc failed with the following command line call:\\n{}'.format(command))\n",
      "subprocess.SubprocessError: neuron-cc failed with the following command line call:\n",
      "/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/398/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/398/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 3, 32, 128], \"float32\"], \"1:0\": [[1, 1, 384], \"float32\"], \"2:0\": [[1, 1, 384], \"float32\"], \"3:0\": [[1, 1, 384], \"float32\"], \"4:0\": [[1, 1, 384], \"float32\"], \"5:0\": [[1, 1, 384], \"float32\"], \"6:0\": [[1, 1, 384], \"float32\"], \"7:0\": [[1, 1, 384], \"float32\"], \"8:0\": [[1, 1, 384], \"float32\"], \"9:0\": [[1, 1, 384], \"float32\"], \"10:0\": [[1, 1, 384], \"float32\"], \"11:0\": [[1, 1, 384], \"float32\"], \"12:0\": [[1, 1, 384], \"float32\"], \"13:0\": [[1, 1, 384], \"float32\"], \"14:0\": [[1, 1, 384], \"float32\"], \"15:0\": [[1, 1, 384], \"float32\"], \"16:0\": [[1, 1, 384], \"float32\"]}, \"outputs\": [\"Encoder_12/LayerNorm_12/aten_layer_norm/batchnorm/add_1:0\", \"Decoder_132/DecoderLayer_3/aten_add/add:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_222/DecoderLayer_3/aten_add/add:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_312/DecoderLayer_3/aten_add/add:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_402/DecoderLayer_3/aten_add/add:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_492/DecoderLayer_3/aten_add/add:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_582/DecoderLayer_3/aten_add/add:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_672/DecoderLayer_3/aten_add/add:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\", \"Decoder_762/DecoderLayer_3/aten_add/add:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\"]}' --verbose 35\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1374 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/559/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/559/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/559/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"], \"1:0\": [[128, 12, 32], \"float32\"], \"2:0\": [[128, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_transpose_4/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1375 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/561/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/561/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/561/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"], \"1:0\": [[128, 12, 32], \"float32\"], \"2:0\": [[128, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_transpose_4/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1376 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/563/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/563/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/563/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"], \"1:0\": [[128, 12, 32], \"float32\"], \"2:0\": [[128, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_transpose_4/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1377 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/565/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/565/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/565/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"], \"1:0\": [[128, 12, 32], \"float32\"], \"2:0\": [[128, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_transpose_4/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1378 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/567/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/567/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/567/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"], \"1:0\": [[128, 12, 32], \"float32\"], \"2:0\": [[128, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_transpose_4/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1379 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/569/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/569/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/569/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"], \"1:0\": [[128, 12, 32], \"float32\"], \"2:0\": [[128, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_transpose_4/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1380 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/571/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/571/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/571/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"], \"1:0\": [[128, 12, 32], \"float32\"], \"2:0\": [[128, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_transpose_4/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1381 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/573/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/573/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/573/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 12, 32], \"float32\"], \"1:0\": [[128, 12, 32], \"float32\"], \"2:0\": [[128, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_transpose_4/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1382 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/575/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/575/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/575/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_132/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1383 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/579/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/579/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/579/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_222/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1384 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/583/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/583/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/583/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_312/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1385 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/587/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/587/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/587/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_402/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1386 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/591/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/591/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/591/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_492/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1387 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/595/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/595/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/595/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_582/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1388 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/599/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/599/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/599/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_672/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1389 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/603/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/603/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/603/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 384], \"float32\"]}, \"outputs\": [\"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_762/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:PyTorch to TF conversion failed to resolve function on aten::cumsum with inputs [<tf.Tensor 'aten_eq/Equal:0' shape=(1, 8) dtype=bool>, -1, None]\n",
      "INFO:Neuron:Exception = Expected bool, got 0 of type 'int' instead.\n",
      "WARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$1390; falling back to native python function call\n",
      "ERROR:Neuron:Expected bool, got 0 of type 'int' instead.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 324, in _AssertCompatible\n",
      "    fn(values)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 263, in inner\n",
      "    _ = [_check_failed(v) for v in nest.flatten(values)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 264, in <listcomp>\n",
      "    if not isinstance(v, expected_types)]\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 248, in _check_failed\n",
      "    raise ValueError(v)\n",
      "ValueError: 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/convert.py\", line 392, in op_converter\n",
      "    item, inputs, compiler_workdir=sg_workdir, **kwargs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/decorators.py\", line 82, in trace\n",
      "    transform_torch_graph_to_tensorflow(jit_trace, example_inputs, neuron_graph=func, **kwargs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/decorators.py\", line 583, in transform_torch_graph_to_tensorflow\n",
      "    raise e\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/decorators.py\", line 577, in transform_torch_graph_to_tensorflow\n",
      "    tensor_outputs = local_func(op, *tensor_inputs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/ops/aten.py\", line 1515, in cumsum\n",
      "    prev = tf.constant([0], dtype=dtype)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 161, in constant_v1\n",
      "    allow_broadcast=False)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 265, in _constant_impl\n",
      "    allow_broadcast=allow_broadcast))\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 449, in make_tensor_proto\n",
      "    _AssertCompatible(values, dtype)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 331, in _AssertCompatible\n",
      "    (dtype.name, repr(mismatch), type(mismatch).__name__))\n",
      "TypeError: Expected bool, got 0 of type 'int' instead.\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1391 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/610/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/610/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/610/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 8], \"int64\"], \"1:0\": [[1, 1, 384], \"float32\"], \"tensor.1:0\": [[1, 26, 384], \"float32\"], \"3:0\": [[1, 7, 384], \"float32\"], \"4:0\": [[1, 26, 384], \"float32\"]}, \"outputs\": [\"4:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compile command returned: 1\n",
      "WARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$1391; falling back to native python function call\n",
      "ERROR:Neuron:neuron-cc failed with the following command line call:\n",
      "/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/610/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/610/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 8], \"int64\"], \"1:0\": [[1, 1, 384], \"float32\"], \"tensor.1:0\": [[1, 26, 384], \"float32\"], \"3:0\": [[1, 7, 384], \"float32\"], \"4:0\": [[1, 26, 384], \"float32\"]}, \"outputs\": [\"4:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\"]}' --verbose 35\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/convert.py\", line 392, in op_converter\n",
      "    item, inputs, compiler_workdir=sg_workdir, **kwargs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/decorators.py\", line 229, in trace\n",
      "    'neuron-cc failed with the following command line call:\\n{}'.format(command))\n",
      "subprocess.SubprocessError: neuron-cc failed with the following command line call:\n",
      "/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/610/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/610/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 8], \"int64\"], \"1:0\": [[1, 1, 384], \"float32\"], \"tensor.1:0\": [[1, 26, 384], \"float32\"], \"3:0\": [[1, 7, 384], \"float32\"], \"4:0\": [[1, 26, 384], \"float32\"]}, \"outputs\": [\"4:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_1/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_2/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_div/floordiv:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_3/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul_1/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_chunk/split:1\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach_4/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul_2/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_mul_3/mul:0\"]}' --verbose 35\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1392 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/622/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/622/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/622/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[26, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_div/truediv:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1393 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/627/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/627/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/627/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[8, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:PyTorch to TF conversion failed to resolve function on aten::masked_fill with inputs [<tf.Tensor 'Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_unsqueeze/ExpandDims:0' shape=(1, 26, 8) dtype=float32>, <tf.Tensor 'Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_reshape/Reshape:0' shape=(12, 1, 8) dtype=bool>, -3.3895313892515355e+38]\n",
      "INFO:Neuron:Exception = Dimensions must be equal, but are 12 and 1 for 'Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_masked_fill/BroadcastTo' (op: 'BroadcastTo') with input shapes: [12,1,8], [3] and with input tensors computed as partial shapes: input[1] = [1,26,8].\n",
      "WARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$1394; falling back to native python function call\n",
      "ERROR:Neuron:Dimensions must be equal, but are 12 and 1 for 'Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_masked_fill/BroadcastTo' (op: 'BroadcastTo') with input shapes: [12,1,8], [3] and with input tensors computed as partial shapes: input[1] = [1,26,8].\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1607, in _create_c_op\n",
      "    c_op = c_api.TF_FinishOperation(op_desc)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 12 and 1 for 'Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_masked_fill/BroadcastTo' (op: 'BroadcastTo') with input shapes: [12,1,8], [3] and with input tensors computed as partial shapes: input[1] = [1,26,8].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/convert.py\", line 392, in op_converter\n",
      "    item, inputs, compiler_workdir=sg_workdir, **kwargs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/decorators.py\", line 82, in trace\n",
      "    transform_torch_graph_to_tensorflow(jit_trace, example_inputs, neuron_graph=func, **kwargs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/decorators.py\", line 583, in transform_torch_graph_to_tensorflow\n",
      "    raise e\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/decorators.py\", line 577, in transform_torch_graph_to_tensorflow\n",
      "    tensor_outputs = local_func(op, *tensor_inputs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/ops/aten.py\", line 1695, in masked_fill\n",
      "    mask = tf.broadcast_to(mask, tensor.shape)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 925, in broadcast_to\n",
      "    \"BroadcastTo\", input=input, shape=shape, name=name)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n",
      "    attrs, op_def, compute_device)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1770, in __init__\n",
      "    control_input_ops)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1610, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Dimensions must be equal, but are 12 and 1 for 'Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_masked_fill/BroadcastTo' (op: 'BroadcastTo') with input shapes: [12,1,8], [3] and with input tensors computed as partial shapes: input[1] = [1,26,8].\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1395 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/639/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/639/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/639/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[8, 12, 32], \"float32\"], \"1:0\": [[12, 26, 8], \"float32\"]}, \"outputs\": [\"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_transpose_1/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1396 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/641/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/641/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/641/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[26, 384], \"float32\"]}, \"outputs\": [\"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_linear/Add:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_14/aten_detach/Const:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1397 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/645/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/645/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/645/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[26, 1, 384], \"float32\"], \"1:0\": [[1, 26, 384], \"float32\"], \"2:0\": [[1, 128, 384], \"float32\"]}, \"outputs\": [\"Decoder_883/DecoderLayer_3/aten_add/add:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compile command returned: 1\n",
      "WARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$1397; falling back to native python function call\n",
      "ERROR:Neuron:neuron-cc failed with the following command line call:\n",
      "/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/645/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/645/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[26, 1, 384], \"float32\"], \"1:0\": [[1, 26, 384], \"float32\"], \"2:0\": [[1, 128, 384], \"float32\"]}, \"outputs\": [\"Decoder_883/DecoderLayer_3/aten_add/add:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\"]}' --verbose 35\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/convert.py\", line 392, in op_converter\n",
      "    item, inputs, compiler_workdir=sg_workdir, **kwargs)\n",
      "  File \"/home/ubuntu/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/decorators.py\", line 229, in trace\n",
      "    'neuron-cc failed with the following command line call:\\n{}'.format(command))\n",
      "subprocess.SubprocessError: neuron-cc failed with the following command line call:\n",
      "/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/645/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/645/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[26, 1, 384], \"float32\"], \"1:0\": [[1, 26, 384], \"float32\"], \"2:0\": [[1, 128, 384], \"float32\"]}, \"outputs\": [\"Decoder_883/DecoderLayer_3/aten_add/add:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_1/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_2/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_div/floordiv:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_3/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul_1/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_chunk/split:1\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach_4/Const:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul_2/mul:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_mul_3/mul:0\"]}' --verbose 35\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1398 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/666/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/666/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/666/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[26, 12, 32], \"float32\"], \"1:0\": [[128, 12, 32], \"float32\"], \"2:0\": [[128, 12, 32], \"float32\"]}, \"outputs\": [\"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_transpose_4/transpose:0\"]} --verbose 35'\n",
      "INFO:Neuron:Compiling function _NeuronGraph$1399 with neuron-cc; log file is at /home/ubuntu/parseq/workdir/668/graph_def.neuron-cc.log\n",
      "INFO:Neuron:Compiling with command line: '/home/ubuntu/env_aws_neuron_pytorch/bin/neuron-cc compile /home/ubuntu/parseq/workdir/668/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /home/ubuntu/parseq/workdir/668/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[26, 384], \"float32\"]}, \"outputs\": [\"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_linear/Add:0\", \"Decoder_883/DecoderLayer_3/MultiheadAttention_19/aten_detach/Const:0\"]} --verbose 35'\n"
     ]
    }
   ],
   "source": [
    "allowed_ops = set(torch.neuron.get_supported_operations())\n",
    "allowed_ops.remove(\"aten::view\")\n",
    "\n",
    "\n",
    "neuron_model = torch.neuron.trace(ts_model, compiler_workdir=\"./workdir\", example_inputs=torch.rand(size=(1, 3, 32, 128)), op_whitelist=allowed_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(neuron_model,'parseq_model_traced_neuron_new.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-Mar-16 23:27:19.0305  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0309  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0313  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0318  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0322  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0326  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0330  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0335  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0339  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0343  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0347  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0351  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0356  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0360  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0364  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0368  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0373  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0377  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0381  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0385  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0389  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0394  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0398  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0402  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0406  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0410  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0414  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0419  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0423  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0427  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0431  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0435  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0439  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0444  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0448  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0452  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0456  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0460  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0464  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0469  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0473  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0477  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0481  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0486  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0490  7392:7392  ERROR   HAL:aws_hal_tpb_pooling_write_profile       failed programming the engine\n",
      "\n",
      "2023-Mar-16 23:27:19.0490  7392:7392  ERROR  TDRV:tpb_eng_init_hals_v1                    NC 0 init failed!\n",
      "\n",
      "2023-Mar-16 23:27:19.0490  7392:7392  ERROR  TDRV:tdrv_init_one_mla_phase2                nd0 nc0 HAL init failed. error:1\n",
      "2023-Mar-16 23:27:19.0490  7392:7392  ERROR  TDRV:notification_destroy                    Notifications not initialized! \n",
      "2023-Mar-16 23:27:19.0490  7392:7392  ERROR  TDRV:notification_destroy                    Notifications not initialized! \n",
      "2023-Mar-16 23:27:19.0490  7392:7392  ERROR  TDRV:notification_destroy                    Notifications not initialized! \n",
      "2023-Mar-16 23:27:19.0490  7392:7392  ERROR  TDRV:notification_destroy                    Notifications not initialized! \n",
      "2023-Mar-16 23:27:20.0538  7392:7392  ERROR  TDRV:tdrv_destroy                            TDRV not initialized\n",
      "2023-Mar-16 23:27:20.0538  7392:7392  ERROR   NRT:nrt_init                                Failed to initialize devices, error:1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The PyTorch Neuron Runtime could not be initialized. Neuron Driver issues are logged\nto your system logs. See the Neuron Runtime's troubleshooting guide for help on this\ntopic: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7392/793316231.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneuron_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parseq_model_traced_neuron_new.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch_neuron/jit_load_wrapper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mscript_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjit_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mfound_neuron_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscript_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env_aws_neuron_pytorch/lib/python3.7/site-packages/torch/jit/_serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, _extra_files)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mcu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompilationUnit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mcpp_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_ir_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_extra_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         cpp_module = torch._C.import_ir_module_from_buffer(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The PyTorch Neuron Runtime could not be initialized. Neuron Driver issues are logged\nto your system logs. See the Neuron Runtime's troubleshooting guide for help on this\ntopic: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/"
     ]
    }
   ],
   "source": [
    "neuron_model = torch.jit.load('parseq_model_traced_neuron_new.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_parallel = torch.neuron.DataParallel(neuron_model)\n",
    "\n",
    "img_folder = \"/home/ubuntu/parseq/digits_demo\"\n",
    "\n",
    "input_holder = torch.zeros(batch_size, 3, 32, 128)\n",
    "\n",
    "img_paths = [os.path.join(img_folder, x) for x in os.listdir(img_folder) if x.endswith(\"jpg\")]\n",
    "\n",
    "N = len(img_paths)\n",
    "n_batch = N//batch_size + 1\n",
    "\n",
    "for i in range(n_batch):\n",
    "    # print(list(range((i-1)*batch_size,batch_size*i)))\n",
    "    if batch_size*(i+1) < N:\n",
    "        input_holder = torch.zeros(batch_size, 3, 32, 128)\n",
    "        img_paths_tmp = img_paths[(i)*batch_size:batch_size*(i+1)]\n",
    "    else:\n",
    "        input_holder = torch.zeros(N-(n_batch-1)*batch_size, 3, 32, 128)\n",
    "        img_paths_tmp = img_paths[(i)*batch_size:N]\n",
    "    \n",
    "    print(input_holder.size())\n",
    "    for j in range(len(img_paths_tmp)):\n",
    "                \n",
    "        img_input = Image.open(img_paths_tmp[j]).convert('RGB')\n",
    "        img_input = preprocess_parseq(img_input.convert('RGB'))\n",
    "        # Preprocess. Model expects a batch of images with shape: (B, C, H, W)\n",
    "        \n",
    "        input_holder[j, :, :, :] = img_input[:,:,:]\n",
    "        \n",
    "    start = time.time()\n",
    "    logits_neuron = model_parallel(input_holder)\n",
    "    end_1 = time.time()\n",
    "    logits = parseq(input_holder)\n",
    "    end_2 = time.time()\n",
    "\n",
    "    # print(\"parseq\")\n",
    "    # print(end_2-end_1)\n",
    "    # print(\"torchscript\")\n",
    "    # print(end_1-start)\n",
    "    pred = logits.softmax(-1)\n",
    "    label, confidence = parseq.tokenizer.decode(pred)\n",
    "        \n",
    "    pred = logits_neuron.softmax(-1)\n",
    "    label_neuron, confidence = parseq.tokenizer.decode(pred)\n",
    "    \n",
    "    print(label_neuron, label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuron_pytorch_p36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
