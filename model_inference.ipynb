{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "preprocess_parseq = T.Compose([\n",
    "            T.Resize((32, 128), T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(0.5, 0.5)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/baudm_parseq_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/OCR_research/env_ocr_research/lib/python3.7/site-packages/torch/nn/modules/module.py:1130: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at  ../torch/csrc/jit/codegen/cuda/parser.cpp:3513.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['448', '398', '464', '422'] ['448', '398', '464', '422']\n",
      "torch.Size([4, 3, 32, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/OCR_research/env_ocr_research/lib/python3.7/site-packages/torch/nn/modules/module.py:1130: UserWarning: operator() profile_node %2083 : int[] = prim::profile_ivalue(%2081)\n",
      " does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n",
      "  return forward_call(*input, **kwargs)\n",
      "/home/ubuntu/OCR_research/env_ocr_research/lib/python3.7/site-packages/torch/nn/modules/module.py:1130: UserWarning: concrete shape for linear input & weight are required to decompose into matmul + bias (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:2076.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['464', '597', '3.34', '3.77'] ['464', '597', '3.34', '3.77']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['597', \"'13N\", '498', '647'] ['597', \"'13N\", '498', '647']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['314', '4#', '224', '348'] ['314', '4#', '224', '348']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['464', '427', '698', '464'] ['464', '427', '698', '464']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['458', '558', '1087', '464'] ['458', '558', '1087', '464']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['297', '472', '298', '98'] ['297', '472', '298', '98']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['674', '387', '422', '467'] ['674', '387', '422', '467']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['538', '538', '547', '397'] ['538', '538', '547', '397']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['474', '497', '997', '497'] ['474', '497', '997', '497']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['224', '417', '417', '538'] ['224', '417', '417', '538']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['497', '494', '497', '$496'] ['497', '494', '497', '$496']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['1897', '5.00', '$224', '594'] ['1897', '5.00', '$224', '594']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['382', '538', '697', '414'] ['382', '538', '697', '414']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['548', '498', '2087', '1547'] ['548', '498', '2087', '1547']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['597', '1097', '$422', '448'] ['597', '1097', '$422', '448']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['747', '47', '548', '377'] ['747', '47', '548', '377']\n",
      "torch.Size([4, 3, 32, 128])\n",
      "['647', '1997', '497', '497'] ['647', '1997', '497', '497']\n",
      "torch.Size([1, 3, 32, 128])\n",
      "['998'] ['998']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "parseq = torch.hub.load('baudm/parseq', 'parseq', pretrained=True).eval()\n",
    "\n",
    "output_path = \"parseq_{}_b{}_torchscript.pt\".format(\"cpu\", batch_size)\n",
    "ts_model = torch.jit.load(output_path)\n",
    "img_folder = \"/home/ubuntu/parseq/digits_demo\"\n",
    "\n",
    "input_holder = torch.zeros(batch_size, 3, 32, 128)\n",
    "\n",
    "img_paths = [os.path.join(img_folder, x) for x in os.listdir(img_folder) if x.endswith(\"jpg\")]\n",
    "\n",
    "N = len(img_paths)\n",
    "n_batch = N//batch_size + 1\n",
    "\n",
    "for i in range(n_batch):\n",
    "    # print(list(range((i-1)*batch_size,batch_size*i)))\n",
    "    if batch_size*(i+1) < N:\n",
    "        input_holder = torch.zeros(batch_size, 3, 32, 128)\n",
    "        img_paths_tmp = img_paths[(i)*batch_size:batch_size*(i+1)]\n",
    "    else:\n",
    "        input_holder = torch.zeros(N-(n_batch-1)*batch_size, 3, 32, 128)\n",
    "        img_paths_tmp = img_paths[(i)*batch_size:N]\n",
    "    \n",
    "    print(input_holder.size())\n",
    "    for j in range(len(img_paths_tmp)):\n",
    "                \n",
    "        img_input = Image.open(img_paths_tmp[j]).convert('RGB')\n",
    "        img_input = preprocess_parseq(img_input.convert('RGB'))\n",
    "        # Preprocess. Model expects a batch of images with shape: (B, C, H, W)\n",
    "        \n",
    "        input_holder[j, :, :, :] = img_input[:,:,:]\n",
    "        \n",
    "    with torch.no_grad():  \n",
    "        start = time.time()\n",
    "        logits_ts = ts_model(input_holder)\n",
    "        end_1 = time.time()\n",
    "        logits = parseq(input_holder)\n",
    "        end_2 = time.time()\n",
    "\n",
    "    # print(\"parseq\")\n",
    "    # print(end_2-end_1)\n",
    "    # print(\"torchscript\")\n",
    "    # print(end_1-start)\n",
    "    pred = logits.softmax(-1)\n",
    "    label, confidence = parseq.tokenizer.decode(pred)\n",
    "        \n",
    "    pred = logits_ts.softmax(-1)\n",
    "    label_ts, confidence = parseq.tokenizer.decode(pred)\n",
    "    \n",
    "    print(label_ts, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# reading configuration\n",
    "charset = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
    "BOS = '[B]'\n",
    "EOS = '[E]'\n",
    "PAD = '[P]'\n",
    "specials_first = (EOS,)\n",
    "specials_last = (BOS, PAD)\n",
    "itos = specials_first + tuple(charset) + specials_last\n",
    "stoi = {s: i for i, s in enumerate(itos)}\n",
    "eos_id, bos_id, pad_id = [stoi[s] for s in specials_first + specials_last]\n",
    "itos = specials_first + tuple(charset) + specials_last\n",
    "\n",
    "# decode the model output\n",
    "def tokenizer_filter(probs, ids):\n",
    "    ids = ids.tolist()\n",
    "    try:\n",
    "        eos_idx = ids.index(eos_id)\n",
    "    except ValueError:\n",
    "        eos_idx = len(ids)  # Nothing to truncate.\n",
    "    # Truncate after EOS\n",
    "    ids = ids[:eos_idx]\n",
    "    probs = probs[:eos_idx + 1]  # but include prob. for EOS (if it exists)\n",
    "    return probs, ids\n",
    "\n",
    "def ids2tok(token_ids):\n",
    "    tokens = [itos[i] for i in token_ids]\n",
    "    return ''.join(tokens)\n",
    "\n",
    "def decode(token_dists):\n",
    "    \"\"\"Decode a batch of token distributions.\n",
    "    Args:\n",
    "        token_dists: softmax probabilities over the token distribution. Shape: N, L, C\n",
    "        raw: return unprocessed labels (will return list of list of strings)\n",
    "\n",
    "    Returns:\n",
    "        list of string labels (arbitrary length) and\n",
    "        their corresponding sequence probabilities as a list of Tensors\n",
    "    \"\"\"\n",
    "    batch_tokens = []\n",
    "    batch_probs = []\n",
    "    for dist in token_dists:\n",
    "        probs, ids = dist.max(-1)  # greedy selection\n",
    "        probs, ids = tokenizer_filter(probs, ids)\n",
    "        tokens = ids2tok(ids)\n",
    "        batch_tokens.append(tokens)\n",
    "        batch_probs.append(probs)\n",
    "    return batch_tokens, batch_probs\n",
    "\n",
    "\n",
    "def img_preprocess(img_orig, device=torch.device(\"cpu\"), nH=32, nW=128):\n",
    "    # img_orig is a numpy array from cv2.imread(img_path)\n",
    "    original_image = img_orig[:, :, ::-1]\n",
    "\n",
    "    resized_image = cv2.resize(original_image, [nW, nH], interpolation=cv2.INTER_CUBIC)\n",
    "    resized_image = (resized_image - 0.5 * 255) / (0.5 * 255)\n",
    "\n",
    "    image = torch.as_tensor(resized_image.astype(\"float32\").transpose(2, 0, 1)).unsqueeze(0).to(device)\n",
    "\n",
    "    return image\n",
    "\n",
    "def batch_inference(images, model):\n",
    "\n",
    "    # pre process those images to resize them to the tensor input\n",
    "    input_tensors = torch.zeros([len(images), 3, 32, 128])\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        img_tensor = img_preprocess(image)\n",
    "\n",
    "        input_tensors[i, :,:,:] = img_tensor\n",
    "\n",
    "    # input_tensors = torch.tensor(input_tensors)\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        start = time.time()\n",
    "        logits = model(input_tensors)\n",
    "        end = time.time()\n",
    "\n",
    "    print(\"parseq\")\n",
    "    print(end-start)\n",
    "\n",
    "    pred = logits.softmax(-1)\n",
    "    label, confidence = decode(pred)\n",
    "        \n",
    "    return label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [os.path.join(img_folder, x) for x in os.listdir(img_folder) if x.endswith(\"jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parseq\n",
      "1.1553549766540527\n",
      "['448', '398', '464', '422', '464']\n"
     ]
    }
   ],
   "source": [
    "images = [cv2.imread(x) for x in img_paths[:5]]\n",
    "ts_model = torch.jit.load(\"parseq_cpu_b4_torchscript.pt\")\n",
    "\n",
    "\n",
    "labels = batch_inference(images, ts_model)\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ocr_research",
   "language": "python",
   "name": "env_ocr_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
