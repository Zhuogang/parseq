{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "preprocess_parseq = T.Compose([\n",
    "            T.Resize((32, 128), T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(0.5, 0.5)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(batch_size, device_type = \"cpu\", img_folder = \"/home/ubuntu/parseq/demo_images\"):\n",
    "    if device_type == \"cpu\":\n",
    "        device = torch.device(\"cpu\")\n",
    "        parseq = torch.hub.load('baudm/parseq', 'parseq', pretrained=True).eval()\n",
    "    elif device_type == \"cuda\":\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        parseq = torch.hub.load('baudm/parseq', 'parseq', pretrained=True).eval().cuda()\n",
    "\n",
    "    test_img_path = \"demo_images/art-01107.jpg\"\n",
    "    img = Image.open(test_img_path).convert('RGB')\n",
    "    # Preprocess. Model expects a batch of images with shape: (B, C, H, W)\n",
    "    img = preprocess_parseq(img.convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = parseq(img)\n",
    "    logits.shape  # torch.Size([1, 26, 95]), 94 characters + [EOS] symbol\n",
    "\n",
    "    # Greedy decoding\n",
    "    pred = logits.softmax(-1)\n",
    "    label, confidence = parseq.tokenizer.decode(pred)\n",
    "    print(confidence)\n",
    "    print('Decoded label = {}'.format(label[0]))\n",
    "\n",
    "    dummy_input = torch.rand(batch_size, 3, 32, 128) \n",
    "\n",
    "    output_path = \"parseq_{}_torchscript.pt\"\n",
    "\n",
    "    # To ONNX\n",
    "    parseq.to_torchscript(file_path=output_path, method=\"script\", example_inputs=dummy_input)  # opset v14 or newer is required\n",
    "    \n",
    "    ts_model = torch.jit.load(output_path)\n",
    "    logits = ts_model(img)\n",
    "    logits.shape  # torch.Size([1, 26, 95]), 94 characters + [EOS] symbol\n",
    "    print(logits.shape)\n",
    "    # Greedy decoding\n",
    "    pred = logits.softmax(-1)\n",
    "    label, confidence = parseq.tokenizer.decode(pred)\n",
    "    print('Decoded label = {}'.format(label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_batch(batch_size=1, device_type=\"cuda\", img_folder=\"/home/ubuntu/parseq/digits_demo\", img_type=\"jpg\"):\n",
    "    if device_type == \"cpu\":\n",
    "        device = torch.device(\"cpu\")\n",
    "        parseq = torch.hub.load('baudm/parseq', 'parseq', pretrained=True).eval()\n",
    "    elif device_type == \"cuda\":\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        parseq = torch.hub.load('baudm/parseq', 'parseq', pretrained=True).eval().cuda()\n",
    "\n",
    "    test_img_path = \"demo_images/art-01107.jpg\"\n",
    "    img = Image.open(test_img_path).convert('RGB')\n",
    "    # Preprocess. Model expects a batch of images with shape: (B, C, H, W)\n",
    "    img = preprocess_parseq(img.convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "    logits = parseq(img)\n",
    "    logits.shape  # torch.Size([1, 26, 95]), 94 characters + [EOS] symbol\n",
    "\n",
    "    # Greedy decoding\n",
    "    pred = logits.softmax(-1)\n",
    "    label, confidence = parseq.tokenizer.decode(pred)\n",
    "    print(confidence)\n",
    "    print('Decoded label = {}'.format(label[0]))\n",
    "\n",
    "    dummy_input = torch.rand(batch_size, 3, 32, 128) \n",
    "\n",
    "    output_path = \"parseq_{}_b{}_torchscript.pt\".format(device_type, batch_size)\n",
    "\n",
    "    # To ONNX\n",
    "    parseq.to_torchscript(file_path=output_path, method=\"script\", example_inputs=dummy_input)  # opset v14 or newer is required\n",
    "    \n",
    "    ts_model = torch.jit.load(output_path)\n",
    "    logits = ts_model(img)\n",
    "    logits.shape  # torch.Size([1, 26, 95]), 94 characters + [EOS] symbol\n",
    "    print(logits.shape)\n",
    "    # Greedy decoding\n",
    "    pred = logits.softmax(-1)\n",
    "    label, confidence = parseq.tokenizer.decode(pred)\n",
    "    print('Decoded label = {}'.format(label[0]))\n",
    "    c = 0\n",
    "    x = 0\n",
    "    if os.path.exists(img_folder):\n",
    "        for filename in tqdm(os.listdir(img_folder)):\n",
    "            if filename.endswith(img_type):\n",
    "                x += 1\n",
    "                img_path = os.path.join(img_folder, filename)\n",
    "                # print(img_path)\n",
    "                \n",
    "                img_input = Image.open(img_path).convert('RGB')\n",
    "                # Preprocess. Model expects a batch of images with shape: (B, C, H, W)\n",
    "                img_input = preprocess_parseq(img_input.convert('RGB')).unsqueeze(0).to(device)\n",
    "                # print(img_input)\n",
    "                with torch.no_grad():  \n",
    "                    logits_ts = ts_model(img_input)\n",
    "                    logits = parseq(img_input)\n",
    "\n",
    "                pred = logits_ts.softmax(-1)\n",
    "                label_ts, confidence = parseq.tokenizer.decode(pred)\n",
    "                # print(confidence)\n",
    "                # print('Decoded label = {}'.format(label_ts[0]))\n",
    "                \n",
    "                pred = logits.softmax(-1)\n",
    "                label, confidence = parseq.tokenizer.decode(pred)\n",
    "                # print(confidence)\n",
    "                # print('Decoded label = {}'.format(label[0]))\n",
    "                \n",
    "                \n",
    "                if label[0] == label_ts[0]:\n",
    "                    print(\"matched\")\n",
    "                    c += 1\n",
    "                else:\n",
    "                    print(label[0])\n",
    "                    print(label_ts[0])\n",
    "                    # c += 1\n",
    "    print(c, x)\n",
    "                \n",
    "                # reading = \"\".join([label[0][i] for i in range(len(label[0])) if confidence[0][i] > eps])\n",
    "                # print(reading)\n",
    "                # if len(reading) > 0:\n",
    "                #     outfilename = os.path.join(img_path.replace(img_type, \"txt\"))\n",
    "                #     print(outfilename)\n",
    "                #     outfile = open(outfilename, \"w\")\n",
    "                #     outfile.write(reading)\n",
    "                #     outfile.close()\n",
    "                # return reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/baudm_parseq_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.9979, 0.9996, 0.9999, 0.9998, 0.9868, 0.9998, 0.9432, 0.9765, 0.9937,\n",
      "        0.9981], device='cuda:0', grad_fn=<SliceBackward0>)]\n",
      "Decoded label = CHEWBACCA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/OCR_research/env_ocr_research/lib/python3.7/site-packages/torch/jit/_recursive.py:239: UserWarning: 'norm' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
      "  \" but it is a non-constant {}. Consider removing it.\".format(name, hint))\n",
      "/home/ubuntu/OCR_research/env_ocr_research/lib/python3.7/site-packages/torch/nn/modules/module.py:1130: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at  ../torch/csrc/jit/codegen/cuda/parser.cpp:3513.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 26, 95])\n",
      "Decoded label = CHEWBACCA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/73 [00:00<?, ?it/s]/home/ubuntu/OCR_research/env_ocr_research/lib/python3.7/site-packages/torch/nn/modules/module.py:1130: UserWarning: FALLBACK path has been taken inside: runCudaFusionGroup. This is an indication that codegen Failed for some reason.\n",
      "To debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`\n",
      " (Triggered internally at  ../torch/csrc/jit/codegen/cuda/manager.cpp:329.)\n",
      "  return forward_call(*input, **kwargs)\n",
      "  1%|▏         | 1/73 [00:03<04:09,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 6/73 [00:08<01:10,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 14/73 [00:09<00:18,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 22/73 [00:09<00:07,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 30/73 [00:09<00:03, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 34/73 [00:09<00:02, 15.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 42/73 [00:09<00:01, 21.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 50/73 [00:10<00:00, 26.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 58/73 [00:10<00:00, 29.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 62/73 [00:10<00:00, 30.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 70/73 [00:10<00:00, 31.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n",
      "matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:10<00:00,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "73 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main_batch(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "66//8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/baudm_parseq_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 32, 128])\n",
      "parseq\n",
      "0.5632619857788086\n",
      "torchscript\n",
      "1.0032198429107666\n",
      "['448', '398', '464', '422', '464', '597', '3.34', '3.77', '597', \"'13N\", '498', '647', '314', '4#', '224', '348'] ['448', '398', '464', '422', '464', '597', '3.34', '3.77', '597', \"'13N\", '498', '647', '314', '4#', '224', '348']\n",
      "torch.Size([16, 3, 32, 128])\n",
      "parseq\n",
      "0.5741300582885742\n",
      "torchscript\n",
      "18.885210752487183\n",
      "['464', '427', '698', '464', '458', '558', '1087', '464', '297', '472', '298', '98', '674', '387', '422', '467'] ['464', '427', '698', '464', '458', '558', '1087', '464', '297', '472', '298', '98', '674', '387', '422', '467']\n",
      "torch.Size([16, 3, 32, 128])\n",
      "parseq\n",
      "0.5603926181793213\n",
      "torchscript\n",
      "0.5546107292175293\n",
      "['538', '538', '547', '397', '474', '497', '997', '497', '224', '417', '417', '538', '497', '494', '497', '$496'] ['538', '538', '547', '397', '474', '497', '997', '497', '224', '417', '417', '538', '497', '494', '497', '$496']\n",
      "torch.Size([16, 3, 32, 128])\n",
      "parseq\n",
      "0.560748815536499\n",
      "torchscript\n",
      "0.5607671737670898\n",
      "['1897', '5.00', '$224', '594', '382', '538', '697', '414', '548', '498', '2087', '1547', '597', '1097', '$422', '448'] ['1897', '5.00', '$224', '594', '382', '538', '697', '414', '548', '498', '2087', '1547', '597', '1097', '$422', '448']\n",
      "torch.Size([9, 3, 32, 128])\n",
      "parseq\n",
      "0.32965660095214844\n",
      "torchscript\n",
      "0.3203697204589844\n",
      "['747', '47', '548', '377', '647', '1997', '497', '497', '998'] ['747', '47', '548', '377', '647', '1997', '497', '497', '998']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "parseq = torch.hub.load('baudm/parseq', 'parseq', pretrained=True).eval()\n",
    "\n",
    "output_path = \"parseq_{}_b{}_torchscript.pt\".format(\"cpu\", batch_size)\n",
    "ts_model = torch.jit.load(output_path)\n",
    "img_folder = \"/home/ubuntu/parseq/digits_demo\"\n",
    "\n",
    "input_holder = torch.zeros(batch_size, 3, 32, 128)\n",
    "\n",
    "img_paths = [os.path.join(img_folder, x) for x in os.listdir(img_folder) if x.endswith(\"jpg\")]\n",
    "\n",
    "N = len(img_paths)\n",
    "n_batch = N//batch_size + 1\n",
    "\n",
    "for i in range(n_batch):\n",
    "    # print(list(range((i-1)*batch_size,batch_size*i)))\n",
    "    if batch_size*(i+1) < N:\n",
    "        input_holder = torch.zeros(batch_size, 3, 32, 128)\n",
    "        img_paths_tmp = img_paths[(i)*batch_size:batch_size*(i+1)]\n",
    "    else:\n",
    "        input_holder = torch.zeros(N-(n_batch-1)*batch_size, 3, 32, 128)\n",
    "        img_paths_tmp = img_paths[(i)*batch_size:N]\n",
    "    \n",
    "    print(input_holder.size())\n",
    "    for j in range(len(img_paths_tmp)):\n",
    "                \n",
    "        img_input = Image.open(img_paths_tmp[j]).convert('RGB')\n",
    "        img_input = preprocess_parseq(img_input.convert('RGB'))\n",
    "        # Preprocess. Model expects a batch of images with shape: (B, C, H, W)\n",
    "        \n",
    "        input_holder[j, :, :, :] = img_input[:,:,:]\n",
    "        \n",
    "    with torch.no_grad():  \n",
    "        start = time.time()\n",
    "        logits_ts = ts_model(input_holder)\n",
    "        end_1 = time.time()\n",
    "        logits = parseq(input_holder)\n",
    "        end_2 = time.time()\n",
    "\n",
    "    # print(\"parseq\")\n",
    "    # print(end_2-end_1)\n",
    "    # print(\"torchscript\")\n",
    "    # print(end_1-start)\n",
    "    pred = logits.softmax(-1)\n",
    "    label, confidence = parseq.tokenizer.decode(pred)\n",
    "        \n",
    "    pred = logits_ts.softmax(-1)\n",
    "    label_ts, confidence = parseq.tokenizer.decode(pred)\n",
    "    \n",
    "    print(label_ts, label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ocr_research",
   "language": "python",
   "name": "env_ocr_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
